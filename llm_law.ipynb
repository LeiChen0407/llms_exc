{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd91490-61c7-4fc0-84d5-5cca77c44957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4pdvGPU Msg(54767:140614090889024:libvgpu.c:869)]: Initializing.....\n",
      "[4pdvGPU Warn(54767:140614090889024:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(54767:140614090889024:hook.c:475)]: remap handles for device 1\n",
      "[4pdvGPU Warn(54767:140614090889024:multiprocess_memory_limit.c:568)]: Kick dead proc 54607\n",
      "[4pdvGPU Warn(54767:140614090889024:multiprocess_memory_limit.c:568)]: Kick dead proc 54699\n",
      "[4pdvGPU Warn(54767:140614090889024:utils.c:228)]: get default cuda 2 from (null)\n",
      "[4pdvGPU Msg(54767:140614090889024:libvgpu.c:902)]: Initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4pdvGPU Msg(54851:140509772027712:libvgpu.c:869)]: Initializing.....\n",
      "[4pdvGPU Warn(54851:140509772027712:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(54851:140509772027712:hook.c:475)]: remap handles for device 1\n",
      "[4pdvGPU Warn(54851:140509772027712:utils.c:228)]: get default cuda 2 from (null)\n",
      "[4pdvGPU Msg(54851:140509772027712:libvgpu.c:902)]: Initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.9: Fast Qwen2 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA A30. Num GPUs = 2. Max memory: 24.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4pdvGPU Msg(54767:140614090889024:memory.c:566)]: orig free=24866193408 total=25217466368 limit=25769803776 usage=234881024\n",
      "[4pdvGPU Msg(54767:140614090889024:memory.c:566)]: orig free=24950079488 total=25217466368 limit=25769803776 usage=234881024\n",
      "Loading checkpoint shards:   0%|                                        | 0/2 [00:00<?, ?it/s][4pdvGPU Msg(54767:140614090889024:memory.c:566)]: orig free=24866193408 total=25217466368 limit=25769803776 usage=234881024\n",
      "[4pdvGPU Msg(54767:140614090889024:memory.c:566)]: orig free=24950079488 total=25217466368 limit=25769803776 usage=234881024\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.61s/it]\n",
      "[4pdvGPU Warn(55053:139988863162176:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(55053:139988863162176:hook.c:475)]: remap handles for device 1\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_8bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../model/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = load_in_8bit,\n",
    "    gpu_memory_utilization=1,\n",
    "    device_map='auto'\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0879c7-d89d-4fa9-9431-9a3cc48eb5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3218360-be4e-414f-8267-65bf4cb70e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214408/214408 [00:07<00:00, 28101.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts}\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"/mnt/proj/data/normal/\",split=\"train\")\n",
    "dataset = dataset.shuffle(seed=46)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2accb236-f5cc-45b2-b161-a3f3fe8d8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e77d32-dff7-49fb-8d59-b5e8c0bebe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'leg_case_cls-25',\n",
       " 'input': 'ä»¥ä¸‹æ–‡æœ¬åæ˜ äº†ä»€ä¹ˆç±»å‹çš„é£é™©\\næ±Ÿè‹å®¿è¿çš„ç››å…ˆç”Ÿä¸€å®¶,æœ€è¿‘å¾ˆè‹¦æ¼.ä¸ºå•¥?å®¶é‡Œ40ä¸‡çš„å–æˆ¿æ¬¾,è¢«ä¸¤ä¸ªå­©å­èŠ±å¾—ä¸€åˆ†ä¸å‰©,è¿™äº›é’±éƒ½è®©ç†Šå­©å­ç»™å„å¹³å°ä¸»æ’­æ‰“èµäº†!ä½ è¯´æ°”äººä¸?åŸæ¥ç››å…ˆç”Ÿé™ªçˆ¶æ¯åˆ°é“¶è¡ŒåŠä¸šåŠ¡,å‘ç°é“¶è¡Œå¡é‡Œçš„40ä¸‡ä¸ç¿¼è€Œé£!è¿™å¯æ˜¯å…ƒæ—¦å‰å®¶é‡Œå–æˆ¿å­çš„é’±å•Š.ä¸€æŸ¥é“¶è¡Œæµæ°´æ˜¾ç¤º,ä»1æœˆ4æ—¥åˆ°2æœˆ16æ—¥ä¹‹é—´,æ¯å¤©éƒ½æœ‰ä¸‰å››åç¬”æ”¯ä»˜å®æˆ–QQæ”¯ä»˜è½¬è´¦,æ¯ç¬”èµ„é‡‘ä»å‡ å—é’±åˆ°ä¸€åƒå¤šå—é’±ä¸ç­‰,æœ€å¤šçš„ä¸€å¤©æœ‰å››äº”ä¸‡å—é’±.40ä¸‡ä¸æ˜¯ä¸€ä¸ªå°æ•°å­—,å­©å­èŠ±è¿™ä¹ˆå¤šé’±,å¤§äººæ€ä¹ˆä¼šä¸çŸ¥é“å‘¢?æ®ç››å…ˆç”Ÿè®²,ä»–å’Œå¼Ÿå¼Ÿå¸¸å¹´åœ¨å¤–æ‰“å·¥,ä¾„å­å°åŒ—è·Ÿç€å¥¶å¥¶ç”Ÿæ´».å› ä¸ºè€å¹´æœºæ— æ³•ä¸‹è½½ä¹ é¢˜ä½œä¸š,äºæ˜¯ç»™å°åŒ—ä¹°äº†ä¸€éƒ¨æ™ºèƒ½æ‰‹æœº.æœ¬æƒ³ç›‘ç£å­©å­å†™ä½œä¸š,æ²¡æƒ³åˆ°å­©å­ç«Ÿè¿·ä¸Šäº†æ‰‹æ¸¸,è¿˜ç”¨å¥¶å¥¶çš„æ‰‹æœºå·æ³¨å†Œ,å¹¶ä¸”ç»‘å®šäº†é“¶è¡Œå¡.å­©å­æ˜¯å¦‚ä½•å¾—çŸ¥å¯†ç çš„å‘¢?åŸæ¥å¥¶å¥¶å¹´çºªå¤§äº†,å®³æ€•è®°ä¸ä½å¯†ç ,å°±å†™åœ¨äº†ä¸€å¼ çº¸ä¸Š,æ²¡æƒ³åˆ°è¢«å­©å­å‘ç°äº†.éœ€è¦è¾“å…¥éªŒè¯ç çš„æ—¶å€™,æ‹¿èµ·è€äººæ‰‹æœºä¸€çœ‹ä¾¿çŸ¥,è€Œè¿™ä¸€åˆ‡,å­©å­å¥¶å¥¶æ¯«ä¸çŸ¥æƒ…!äºæ˜¯å°åŒ—å’Œå¦å¤–ä¸€ä¸ª13å²çš„å­©å­,ä¸¤äººå¼€èµ·ç‹‚æ¬¢æ¨¡å¼,ç–¯ç‹‚ç»™ä¸»æ’­ä»¬æ‰“èµ.å­©å­åœ¨å®¶æ²¡äº‹å¯å¹²,åŠ ä¸Šå®¶é•¿ç–äºç›‘ç£,ç©æ¸¸æˆä¼šå¯¼è‡´è¡Œä¸ºä¸Šç˜¾.ç»å¸¸ç©æ¸¸æˆçš„äººéƒ½çŸ¥é“,è€è™æœºè¾“çš„æ—¶å€™æ²‰é»˜æ— å£°,èµ¢çš„æ—¶å€™ä¼šæœ‰çº¢ç¯é—ªçƒã€å¥½åƒä½ èµ¢å¾—äº†å…¨ä¸–ç•Œä¸€æ ·.æ‰€ä»¥æ‰ä¼šæœ‰é‚£ä¹ˆå¤šçš„äººè¿·æ‹æ¸¸æˆ,å®ƒç»™äººçš„æ„Ÿå®˜åˆºæ¿€å¤ªå¤§äº†.40ä¸‡,å¯¹äºä¸€ä¸ªå†œæ‘å®¶åº­æ¥è¯´ä¸æ˜¯å°æ•°,çœŸä¸çŸ¥é“çˆ¶æ¯è¦è¾›è‹¦æ”’å¤šå°‘å¹´è¡€æ±—é’±,æ‰èƒ½å‡‘é½è¿™40ä¸‡.ä»¤äººé«˜å…´çš„æ˜¯,å·²ç»æœ‰ä¸€éƒ¨åˆ†å¹³å°ç­”åº”ç»™å°åŒ—ä¸€å®¶é€€é’±äº†.å¸Œæœ›å­©å­å’Œå®¶é•¿éƒ½èƒ½è®°ä½è¿™ä¸ªæ•™è®­.#ä¸¤ç†Šå­©å­ç©æ¸¸æˆæ‰“èµä¸»æ’­èŠ±å…‰40ä¸‡#',\n",
       " 'output': 'ç›—ç”¨é£é™©',\n",
       " 'text': '\\n\\n### Input:\\nä»¥ä¸‹æ–‡æœ¬åæ˜ äº†ä»€ä¹ˆç±»å‹çš„é£é™©\\næ±Ÿè‹å®¿è¿çš„ç››å…ˆç”Ÿä¸€å®¶,æœ€è¿‘å¾ˆè‹¦æ¼.ä¸ºå•¥?å®¶é‡Œ40ä¸‡çš„å–æˆ¿æ¬¾,è¢«ä¸¤ä¸ªå­©å­èŠ±å¾—ä¸€åˆ†ä¸å‰©,è¿™äº›é’±éƒ½è®©ç†Šå­©å­ç»™å„å¹³å°ä¸»æ’­æ‰“èµäº†!ä½ è¯´æ°”äººä¸?åŸæ¥ç››å…ˆç”Ÿé™ªçˆ¶æ¯åˆ°é“¶è¡ŒåŠä¸šåŠ¡,å‘ç°é“¶è¡Œå¡é‡Œçš„40ä¸‡ä¸ç¿¼è€Œé£!è¿™å¯æ˜¯å…ƒæ—¦å‰å®¶é‡Œå–æˆ¿å­çš„é’±å•Š.ä¸€æŸ¥é“¶è¡Œæµæ°´æ˜¾ç¤º,ä»1æœˆ4æ—¥åˆ°2æœˆ16æ—¥ä¹‹é—´,æ¯å¤©éƒ½æœ‰ä¸‰å››åç¬”æ”¯ä»˜å®æˆ–QQæ”¯ä»˜è½¬è´¦,æ¯ç¬”èµ„é‡‘ä»å‡ å—é’±åˆ°ä¸€åƒå¤šå—é’±ä¸ç­‰,æœ€å¤šçš„ä¸€å¤©æœ‰å››äº”ä¸‡å—é’±.40ä¸‡ä¸æ˜¯ä¸€ä¸ªå°æ•°å­—,å­©å­èŠ±è¿™ä¹ˆå¤šé’±,å¤§äººæ€ä¹ˆä¼šä¸çŸ¥é“å‘¢?æ®ç››å…ˆç”Ÿè®²,ä»–å’Œå¼Ÿå¼Ÿå¸¸å¹´åœ¨å¤–æ‰“å·¥,ä¾„å­å°åŒ—è·Ÿç€å¥¶å¥¶ç”Ÿæ´».å› ä¸ºè€å¹´æœºæ— æ³•ä¸‹è½½ä¹ é¢˜ä½œä¸š,äºæ˜¯ç»™å°åŒ—ä¹°äº†ä¸€éƒ¨æ™ºèƒ½æ‰‹æœº.æœ¬æƒ³ç›‘ç£å­©å­å†™ä½œä¸š,æ²¡æƒ³åˆ°å­©å­ç«Ÿè¿·ä¸Šäº†æ‰‹æ¸¸,è¿˜ç”¨å¥¶å¥¶çš„æ‰‹æœºå·æ³¨å†Œ,å¹¶ä¸”ç»‘å®šäº†é“¶è¡Œå¡.å­©å­æ˜¯å¦‚ä½•å¾—çŸ¥å¯†ç çš„å‘¢?åŸæ¥å¥¶å¥¶å¹´çºªå¤§äº†,å®³æ€•è®°ä¸ä½å¯†ç ,å°±å†™åœ¨äº†ä¸€å¼ çº¸ä¸Š,æ²¡æƒ³åˆ°è¢«å­©å­å‘ç°äº†.éœ€è¦è¾“å…¥éªŒè¯ç çš„æ—¶å€™,æ‹¿èµ·è€äººæ‰‹æœºä¸€çœ‹ä¾¿çŸ¥,è€Œè¿™ä¸€åˆ‡,å­©å­å¥¶å¥¶æ¯«ä¸çŸ¥æƒ…!äºæ˜¯å°åŒ—å’Œå¦å¤–ä¸€ä¸ª13å²çš„å­©å­,ä¸¤äººå¼€èµ·ç‹‚æ¬¢æ¨¡å¼,ç–¯ç‹‚ç»™ä¸»æ’­ä»¬æ‰“èµ.å­©å­åœ¨å®¶æ²¡äº‹å¯å¹²,åŠ ä¸Šå®¶é•¿ç–äºç›‘ç£,ç©æ¸¸æˆä¼šå¯¼è‡´è¡Œä¸ºä¸Šç˜¾.ç»å¸¸ç©æ¸¸æˆçš„äººéƒ½çŸ¥é“,è€è™æœºè¾“çš„æ—¶å€™æ²‰é»˜æ— å£°,èµ¢çš„æ—¶å€™ä¼šæœ‰çº¢ç¯é—ªçƒã€å¥½åƒä½ èµ¢å¾—äº†å…¨ä¸–ç•Œä¸€æ ·.æ‰€ä»¥æ‰ä¼šæœ‰é‚£ä¹ˆå¤šçš„äººè¿·æ‹æ¸¸æˆ,å®ƒç»™äººçš„æ„Ÿå®˜åˆºæ¿€å¤ªå¤§äº†.40ä¸‡,å¯¹äºä¸€ä¸ªå†œæ‘å®¶åº­æ¥è¯´ä¸æ˜¯å°æ•°,çœŸä¸çŸ¥é“çˆ¶æ¯è¦è¾›è‹¦æ”’å¤šå°‘å¹´è¡€æ±—é’±,æ‰èƒ½å‡‘é½è¿™40ä¸‡.ä»¤äººé«˜å…´çš„æ˜¯,å·²ç»æœ‰ä¸€éƒ¨åˆ†å¹³å°ç­”åº”ç»™å°åŒ—ä¸€å®¶é€€é’±äº†.å¸Œæœ›å­©å­å’Œå®¶é•¿éƒ½èƒ½è®°ä½è¿™ä¸ªæ•™è®­.#ä¸¤ç†Šå­©å­ç©æ¸¸æˆæ‰“èµä¸»æ’­èŠ±å…‰40ä¸‡#\\n\\n### Response:\\nç›—ç”¨é£é™©<|im_end|>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[190000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb9244d-d411-421c-86ed-9e6a64946dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4pdvGPU Warn(55149:140090057246528:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(55149:140090057246528:hook.c:475)]: remap handles for device 1\n",
      "[4pdvGPU Warn(55150:140226020153152:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(55150:140226020153152:hook.c:475)]: remap handles for device 1\n",
      "Unsloth: Tokenizing [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214408/214408 [00:52<00:00, 4081.84 examples/s]\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3409,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d80a98-43ca-4623-b923-70695049cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1721078-4b14-4eb6-9e29-b44bde46f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model).to(\"cuda:1\") # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"è¯·å½’çº³è¿™ç¯‡æ–‡ä¹¦çš„å¤§è‡´è¦ç‚¹ è«æŒ¯çº¢ä¸é½çˆ±é’ã€é¾™ä¹¦è´µå€Ÿæ¬¾åˆåŒçº çº·ä¸€å®¡æ°‘äº‹åˆ¤å†³ä¹¦ æ²³å—çœæ»‘å¿äººæ°‘æ³•é™¢ æ°‘ äº‹ åˆ¤ å†³ ä¹¦ ï¼ˆ2018ï¼‰è±«0526æ°‘åˆ805å·åŸå‘Šè«æŒ¯çº¢ï¼Œå¥³ï¼Œ1973å¹´3æœˆ3æ—¥ç”Ÿï¼Œæ±‰æ—ã€‚ å§”æ‰˜ä»£ç†äººå­Ÿå¸…ã€æäºšå»·ï¼ˆå®ä¹ ï¼‰ï¼Œæ²³å—å“èª‰å¾‹å¸ˆäº‹åŠ¡æ‰€å¾‹å¸ˆã€‚ è¢«å‘Šé½çˆ±é’ï¼Œå¥³ï¼Œ1968å¹´2æœˆ28æ—¥ç”Ÿï¼Œæ±‰æ—ã€‚ è¢«å‘Šé¾™ä¹¦è´µï¼Œç”·ï¼Œ1964å¹´2æœˆ3æ—¥ç”Ÿï¼Œæ±‰æ—ã€‚ åŸå‘Šè«æŒ¯çº¢è¯‰è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå€Ÿæ¬¾åˆåŒçº çº·ä¸€æ¡ˆï¼Œæœ¬é™¢å—ç†åï¼Œä¾æ³•é€‚ç”¨ç®€æ˜“ç¨‹åºï¼Œå…¬å¼€å¼€åº­è¿›è¡Œäº†å®¡ç†ã€‚ åŸå‘Šè«æŒ¯çº¢çš„å§”æ‰˜ä»£ç†äººå­Ÿå¸…ã€è¢«å‘Šé½çˆ±é’åˆ°åº­å‚åŠ è¯‰è®¼ï¼Œè¢«å‘Šé¾™ä¹¦è´µç»ä¼ ç¥¨ä¼ å”¤æœªåˆ°åº­å‚åŠ è¯‰è®¼ã€‚ æœ¬æ¡ˆç°å·²å®¡ç†ç»ˆç»“ã€‚ åŸå‘Šè«æŒ¯çº¢è¯‰ç§°ï¼Œ2014å¹´11æœˆ11æ—¥ï¼ŒäºŒè¢«å‘Šå‘åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒï¼Œå¹¶å‘åŸå‘Šå‡ºå…·äº†å€Ÿæ¡ä¸€ä»½ã€‚ åç»å¤šæ¬¡å‚¬è¦ï¼Œè¢«å‘Šå‡ä»¥å„ç§ç†ç”±æ¨æ‹–æ²¡æœ‰å¿è¿˜åŸå‘Šã€‚ èµ·è¯‰è¯·æ±‚ä¾æ³•åˆ¤ä»¤è¢«å‘Šå¿è¿˜åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒåŠåˆ©æ¯ï¼› è¯‰è®¼è´¹ç”¨ç”±è¢«å‘Šæ‰¿æ‹…ã€‚ è¢«å‘Šé½çˆ±é’è¾©ç§°ï¼Œå€Ÿæ¬¾å±å®ï¼Œä½†ä¸­é—´åŸå‘Šæ‹‰èµ°3è½¦ç –æŠ˜æŠµå€Ÿæ¬¾0.8ä¸‡å…ƒï¼Œå°šæ¬ 1.2ä¸‡å…ƒï¼Œæ•…è¢«å‘Šç°åœ¨ä¸æ¬ åŸå‘Š2ä¸‡å…ƒé’±ã€‚ è¢«å‘Šé¾™ä¹¦è´µç¼ºå¸­æœªç­”è¾©ã€‚ ç»å®¡ç†æŸ¥æ˜ï¼š2014å¹´11æœˆ11æ—¥ï¼Œè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒå‘åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒï¼Œå¹¶å‡ºå…·å€Ÿæ¡ä¸€ä»½ï¼Œå†…å®¹ä¸ºï¼šâ€œä»Šå€Ÿåˆ°ç°é‡‘è´°ä¸‡å…ƒæ­£ï¿¥20000å…ƒ2014å¹´11æœˆ11æ—¥é½çˆ±é’é¾™ä¹¦è´µã€‚â€ ä¸Šè¿°å€Ÿæ¬¾è‡³ä»Šæœªäºˆå¿è¿˜ã€‚ ä»¥ä¸Šäº‹å®ï¼Œæœ‰åŸå‘Šæäº¤çš„å€Ÿæ¡åŠåŸã€è¢«å‘Šéƒ¨åˆ†åº­å®¡é™ˆè¿°å¯ä»¥è¯å®ï¼Œä¸Šè¿°è¯æ®ç»åº­å®¡è´¨è¯ï¼Œå¯ä»¥ä½œä¸ºè®¤å®šæœ¬æ¡ˆäº‹å®çš„ä¾æ®ã€‚ æœ¬é™¢è®¤ä¸ºï¼Œè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒå‘åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒï¼Œæœ‰åŸå‘Šæä¾›çš„è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µç­¾åç¡®è®¤çš„å€Ÿæ¡äºˆä»¥è¯å®ï¼Œæ•…è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µè´Ÿæœ‰å…±åŒå¿è¿˜çš„è´£ä»»ã€‚ å…³äºåˆ©æ¯ï¼Œå› æœªæ˜ç¡®çº¦å®šï¼Œåº”è‡ªåŸå‘Šä¸»å¼ æƒåˆ©æ—¶å³èµ·è¯‰ä¹‹æ—¥èµ·ï¼ŒæŒ‰å¹´åˆ©ç‡6%è®¡ç®—ã€‚ è¢«å‘Šé½çˆ±é’ä¸»å¼ åŸå‘Šæ‹‰ç –ä¸‰è½¦å³å·²ç»æŠ˜æŠµå€Ÿæ¬¾0.8ä¸‡å…ƒåŠå…¶ä»–è¾©è§£æ„è§ï¼Œå‡è¯æ®ä¸è¶³ï¼Œä¾æ³•ä¸äºˆé‡‡ä¿¡ã€‚ æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•é€šåˆ™ã€‹ç¬¬å…«åå››æ¡ã€ç¬¬ä¸€ç™¾é›¶å…«æ¡ï¼Œã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘äº‹è¯‰è®¼æ³•ã€‹ç¬¬å…­åå››æ¡ã€ç¬¬ä¸€ç™¾å››åå››æ¡çš„è§„å®šï¼Œåˆ¤å†³å¦‚ä¸‹ï¼šè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µäºæœ¬åˆ¤å†³ç”Ÿæ•ˆåäº”æ—¥å†…å…±åŒå¿è¿˜åŸå‘Šè«æŒ¯çº¢å€Ÿæ¬¾äººæ°‘å¸2ä¸‡å…ƒåŠåˆ©æ¯ï¼ˆè‡ª2018å¹´1æœˆ22æ—¥èµ·ï¼ŒæŒ‰ç…§å¹´åˆ©ç‡6%è®¡ç®—è‡³æœ¬åˆ¤å†³é™å®šçš„å±¥è¡ŒæœŸé™å±Šæ»¡ä¹‹æ—¥æ­¢ï¼‰ã€‚ å¦‚æœæœªæŒ‰æœ¬åˆ¤å†³æŒ‡å®šçš„æœŸé—´å±¥è¡Œç»™ä»˜é‡‘é’±ä¹‰åŠ¡ï¼Œåº”å½“ä¾ç…§ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘äº‹è¯‰è®¼æ³•ã€‹ç¬¬äºŒç™¾äº”åä¸‰æ¡ä¹‹è§„å®šï¼ŒåŠ å€æ”¯ä»˜è¿Ÿå»¶å±¥è¡ŒæœŸé—´çš„å€ºåŠ¡åˆ©æ¯ã€‚ æ¡ˆä»¶å—ç†è´¹300å…ƒï¼Œå‡åŠæ”¶å–150å…ƒï¼Œè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒè´Ÿæ‹…ã€‚ å¦‚ä¸æœæœ¬åˆ¤å†³ï¼Œå¯åœ¨åˆ¤å†³é€è¾¾ä¹‹æ—¥èµ·åäº”æ—¥å†…ï¼Œå‘æœ¬é™¢é€’äº¤ä¸Šè¯‰çŠ¶ï¼Œå¹¶æŒ‰å½“äº‹äººçš„äººæ•°æå‡ºå‰¯æœ¬ï¼Œä¸Šè¯‰äºæ²³å—çœå®‰é˜³å¸‚ä¸­çº§äººæ°‘æ³•é™¢ã€‚ å®¡åˆ¤å‘˜ã€€ã€€å¼ å…´æ”¿ äºŒã€‡ä¸€å…«å¹´äºŒæœˆäºŒåäºŒæ—¥ ä¹¦è®°å‘˜ã€€ã€€ç‹èˆ’èˆ’\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda:1\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e228ed1-30af-4566-a26d-229c68aeab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/lora_3b_full_data/tokenizer_config.json',\n",
       " '../model/lora_3b_full_data/special_tokens_map.json',\n",
       " '../model/lora_3b_full_data/chat_template.jinja',\n",
       " '../model/lora_3b_full_data/vocab.json',\n",
       " '../model/lora_3b_full_data/merges.txt',\n",
       " '../model/lora_3b_full_data/added_tokens.json',\n",
       " '../model/lora_3b_full_data/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../model/lora_3b_full_data\")  # Local saving\n",
    "tokenizer.save_pretrained(\"../model/lora_3b_full_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74aba6dc-4480-4ff1-9615-3fa5811ae07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 321.21 out of 503.19 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:00<00:00, 62.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if True: model.save_pretrained_merged(\"../model/law_3b_full_data_f16\", tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a521d523-b919-4d0d-a152-28057ae1a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4pdvGPU Msg(69350:140645045749568:libvgpu.c:869)]: Initializing.....\n",
      "[4pdvGPU Warn(69350:140645045749568:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(69350:140645045749568:hook.c:475)]: remap handles for device 1\n",
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[4pdvGPU Warn(69350:140645045749568:utils.c:228)]: get default cuda 2 from (null)\n",
      "[4pdvGPU Msg(69350:140645045749568:libvgpu.c:902)]: Initialized\n",
      "[4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=24971051008 total=25217466368 limit=25769803776 usage=236978176\n",
      "[4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=24971051008 total=25217466368 limit=25769803776 usage=236978176\n",
      "Loading checkpoint shards:   0%|                                        | 0/2 [00:00<?, ?it/s][4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=24971051008 total=25217466368 limit=25769803776 usage=236978176\n",
      "[4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=24971051008 total=25217466368 limit=25769803776 usage=236978176\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.62s/it]\n",
      "[4pdvGPU Warn(69455:140280103335744:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(69455:140280103335744:hook.c:475)]: remap handles for device 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: è¿™æ˜¯ä¸€èµ·å€Ÿæ¬¾åˆåŒçº çº·æ¡ˆä»¶ï¼Œæ¶‰åŠåŸå‘Šè«æŒ¯çº¢å’Œè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µã€‚æ ¹æ®æ¡ˆä»¶æè¿°ï¼š\n",
      "\n",
      "åŸå‘Šè«æŒ¯çº¢è¯‰ç§°è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µåœ¨2014å¹´11æœˆ11æ—¥å‘å¥¹å€Ÿæ¬¾2ä¸‡å…ƒï¼Œå¹¶å‡ºå…·äº†ä¸€ä»½å€Ÿæ¡ã€‚\n",
      "\n",
      "è¢«å‘Šé½çˆ±é’è¾©ç§°ï¼Œå€Ÿæ¬¾å±å®ï¼Œä½†æ˜¯åŸå‘Šæ‹‰èµ°äº†ä¸‰è½¦ç –ï¼ŒæŠ˜æŠµäº†å€Ÿæ¬¾ä¸­çš„0.8ä¸‡å…ƒï¼Œæ‰€ä»¥ç›®å‰åªæ¬ åŸå‘Š1.2ä¸‡å…ƒã€‚\n",
      "\n",
      "æ³•é™¢åˆ¤å†³å¦‚ä¸‹ï¼š\n",
      "1. è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒå¿è¿˜å€Ÿæ¬¾2ä¸‡å…ƒï¼›\n",
      "2. è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒæ”¯ä»˜åˆ©æ¯ï¼ˆè‡ªåŸå‘Šèµ·è¯‰ä¹‹æ—¥èµ·ï¼ŒæŒ‰ç…§å¹´åˆ©ç‡6%è®¡ç®—è‡³åˆ¤å†³é™å®šçš„å±¥è¡ŒæœŸé™å±Šæ»¡ä¹‹æ—¥æ­¢ï¼‰ã€‚\n",
      "\n",
      "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¢«å‘Šé½çˆ±é’çš„è¾©è§£æ„è§å¹¶æœªè¢«é‡‡çº³ã€‚\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"../model/law_3b_full_data_f16\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1001f8ea-b103-4780-af30-1f16ac0c079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: D\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "# prompt = \"\"\"\n",
    "# è¯·å½’çº³è¿™ç¯‡æ–‡ä¹¦çš„å¤§è‡´è¦ç‚¹ è«æŒ¯çº¢ä¸é½çˆ±é’ã€é¾™ä¹¦è´µå€Ÿæ¬¾åˆåŒçº çº·ä¸€å®¡æ°‘äº‹åˆ¤å†³ä¹¦ æ²³å—çœæ»‘å¿äººæ°‘æ³•é™¢ æ°‘ äº‹ åˆ¤ å†³ ä¹¦ ï¼ˆ2018ï¼‰è±«0526æ°‘åˆ805å·åŸå‘Šè«æŒ¯çº¢ï¼Œå¥³ï¼Œ1973å¹´3æœˆ3æ—¥ç”Ÿï¼Œæ±‰æ—ã€‚ å§”æ‰˜ä»£\n",
    "# ç†äººå­Ÿå¸…ã€æäºšå»·ï¼ˆå®ä¹ ï¼‰ï¼Œæ²³å—å“èª‰å¾‹å¸ˆäº‹åŠ¡æ‰€å¾‹å¸ˆã€‚ è¢«å‘Šé½çˆ±é’ï¼Œå¥³ï¼Œ1968å¹´2æœˆ28æ—¥ç”Ÿï¼Œæ±‰æ—ã€‚ è¢«å‘Šé¾™ä¹¦è´µï¼Œç”·ï¼Œ1964å¹´2æœˆ3æ—¥ç”Ÿï¼Œæ±‰æ—ã€‚ åŸå‘Šè«æŒ¯çº¢è¯‰è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå€Ÿæ¬¾åˆåŒçº çº·ä¸€æ¡ˆï¼Œæœ¬é™¢å—ç†åï¼Œ\n",
    "# ä¾æ³•é€‚ç”¨ç®€æ˜“ç¨‹åºï¼Œå…¬å¼€å¼€åº­è¿›è¡Œäº†å®¡ç†ã€‚ åŸå‘Šè«æŒ¯çº¢çš„å§”æ‰˜ä»£ç†äººå­Ÿå¸…ã€è¢«å‘Šé½çˆ±é’åˆ°åº­å‚åŠ è¯‰è®¼ï¼Œè¢«å‘Šé¾™ä¹¦è´µç»ä¼ ç¥¨ä¼ å”¤æœªåˆ°åº­å‚åŠ è¯‰è®¼ã€‚ æœ¬æ¡ˆç°å·²å®¡ç†ç»ˆç»“ã€‚ åŸå‘Šè«æŒ¯çº¢è¯‰ç§°ï¼Œ2014å¹´11æœˆ11æ—¥ï¼ŒäºŒè¢«å‘Šå‘åŸå‘Š\n",
    "# å€Ÿæ¬¾2ä¸‡å…ƒï¼Œå¹¶å‘åŸå‘Šå‡ºå…·äº†å€Ÿæ¡ä¸€ä»½ã€‚ åç»å¤šæ¬¡å‚¬è¦ï¼Œè¢«å‘Šå‡ä»¥å„ç§ç†ç”±æ¨æ‹–æ²¡æœ‰å¿è¿˜åŸå‘Šã€‚ èµ·è¯‰è¯·æ±‚ä¾æ³•åˆ¤ä»¤è¢«å‘Šå¿è¿˜åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒåŠåˆ©æ¯ï¼› è¯‰è®¼è´¹ç”¨ç”±è¢«å‘Šæ‰¿æ‹…ã€‚ è¢«å‘Šé½çˆ±é’è¾©ç§°ï¼Œå€Ÿæ¬¾å±å®ï¼Œä½†ä¸­é—´åŸå‘Šæ‹‰èµ°3\n",
    "# è½¦ç –æŠ˜æŠµå€Ÿæ¬¾0.8ä¸‡å…ƒï¼Œå°šæ¬ 1.2ä¸‡å…ƒï¼Œæ•…è¢«å‘Šç°åœ¨ä¸æ¬ åŸå‘Š2ä¸‡å…ƒé’±ã€‚ è¢«å‘Šé¾™ä¹¦è´µç¼ºå¸­æœªç­”è¾©ã€‚ ç»å®¡ç†æŸ¥æ˜ï¼š2014å¹´11æœˆ11æ—¥ï¼Œè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒå‘åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒï¼Œå¹¶å‡ºå…·å€Ÿæ¡ä¸€ä»½ï¼Œå†…å®¹ä¸ºï¼šâ€œä»Šå€Ÿåˆ°ç°é‡‘è´°ä¸‡å…ƒ\n",
    "# æ­£ï¿¥20000å…ƒ2014å¹´11æœˆ11æ—¥é½çˆ±é’é¾™ä¹¦è´µã€‚â€ ä¸Šè¿°å€Ÿæ¬¾è‡³ä»Šæœªäºˆå¿è¿˜ã€‚ ä»¥ä¸Šäº‹å®ï¼Œæœ‰åŸå‘Šæäº¤çš„å€Ÿæ¡åŠåŸã€è¢«å‘Šéƒ¨åˆ†åº­å®¡é™ˆè¿°å¯ä»¥è¯å®ï¼Œä¸Šè¿°è¯æ®ç»åº­å®¡è´¨è¯ï¼Œå¯ä»¥ä½œä¸ºè®¤å®šæœ¬æ¡ˆäº‹å®çš„ä¾æ®ã€‚ æœ¬é™¢è®¤ä¸ºï¼Œè¢«å‘Šé½çˆ±é’ã€\n",
    "# é¾™ä¹¦è´µå…±åŒå‘åŸå‘Šå€Ÿæ¬¾2ä¸‡å…ƒï¼Œæœ‰åŸå‘Šæä¾›çš„è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µç­¾åç¡®è®¤çš„å€Ÿæ¡äºˆä»¥è¯å®ï¼Œæ•…è¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µè´Ÿæœ‰å…±åŒå¿è¿˜çš„è´£ä»»ã€‚ å…³äºåˆ©æ¯ï¼Œå› æœªæ˜ç¡®çº¦å®šï¼Œåº”è‡ªåŸå‘Šä¸»å¼ æƒåˆ©æ—¶å³èµ·è¯‰ä¹‹æ—¥èµ·ï¼ŒæŒ‰å¹´åˆ©ç‡6%è®¡ç®—ã€‚ \n",
    "# è¢«å‘Šé½çˆ±é’ä¸»å¼ åŸå‘Šæ‹‰ç –ä¸‰è½¦å³å·²ç»æŠ˜æŠµå€Ÿæ¬¾0.8ä¸‡å…ƒåŠå…¶ä»–è¾©è§£æ„è§ï¼Œå‡è¯æ®ä¸è¶³ï¼Œä¾æ³•ä¸äºˆé‡‡ä¿¡ã€‚ æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•é€šåˆ™ã€‹ç¬¬å…«åå››æ¡ã€ç¬¬ä¸€ç™¾é›¶å…«æ¡ï¼Œã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘äº‹è¯‰è®¼æ³•ã€‹ç¬¬å…­åå››æ¡ã€ç¬¬ä¸€ç™¾å››åå››æ¡çš„\n",
    "# è§„å®šï¼Œåˆ¤å†³å¦‚ä¸‹ï¼šè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µäºæœ¬åˆ¤å†³ç”Ÿæ•ˆåäº”æ—¥å†…å…±åŒå¿è¿˜åŸå‘Šè«æŒ¯çº¢å€Ÿæ¬¾äººæ°‘å¸2ä¸‡å…ƒåŠåˆ©æ¯ï¼ˆè‡ª2018å¹´1æœˆ22æ—¥èµ·ï¼ŒæŒ‰ç…§å¹´åˆ©ç‡6%è®¡ç®—è‡³æœ¬åˆ¤å†³é™å®šçš„å±¥è¡ŒæœŸé™å±Šæ»¡ä¹‹æ—¥æ­¢ï¼‰ã€‚ å¦‚æœæœªæŒ‰æœ¬åˆ¤å†³æŒ‡å®šçš„æœŸé—´å±¥è¡Œç»™ä»˜\n",
    "# é‡‘é’±ä¹‰åŠ¡ï¼Œåº”å½“ä¾ç…§ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘äº‹è¯‰è®¼æ³•ã€‹ç¬¬äºŒç™¾äº”åä¸‰æ¡ä¹‹è§„å®šï¼ŒåŠ å€æ”¯ä»˜è¿Ÿå»¶å±¥è¡ŒæœŸé—´çš„å€ºåŠ¡åˆ©æ¯ã€‚ æ¡ˆä»¶å—ç†è´¹300å…ƒï¼Œå‡åŠæ”¶å–150å…ƒï¼Œè¢«å‘Šé½çˆ±é’ã€é¾™ä¹¦è´µå…±åŒè´Ÿæ‹…ã€‚ å¦‚ä¸æœæœ¬åˆ¤å†³ï¼Œå¯åœ¨åˆ¤å†³é€è¾¾ä¹‹æ—¥èµ·åäº”\n",
    "# æ—¥å†…ï¼Œå‘æœ¬é™¢é€’äº¤ä¸Šè¯‰çŠ¶ï¼Œå¹¶æŒ‰å½“äº‹äººçš„äººæ•°æå‡ºå‰¯æœ¬ï¼Œä¸Šè¯‰äºæ²³å—çœå®‰é˜³å¸‚ä¸­çº§äººæ°‘æ³•é™¢ã€‚ å®¡åˆ¤å‘˜ã€€ã€€å¼ å…´æ”¿ äºŒã€‡ä¸€å…«å¹´äºŒæœˆäºŒåäºŒæ—¥ ä¹¦è®°å‘˜ã€€ã€€ç‹èˆ’èˆ’\"\"\"\n",
    "prompt = \"\"\"\n",
    "è¿™æ˜¯ä¸€ä¸ªå…³äºæ³•å¾‹é—®é¢˜çš„å•é¡¹é€‰æ‹©é¢˜,è¯·æ ¹æ®é¢˜ç›®é€‰æ‹©æ­£ç¡®çš„é€‰é¡¹(åªéœ€è¦ç»™å‡ºæ­£ç¡®é€‰é¡¹å³å¯)ï¼š\\n\n",
    "é—®é¢˜ï¼šé©¬é”¡äº”åœ¨å®¡ç†â€œæŠ¢äº²æ¡ˆ\"æ—¶å‘ç°ï¼Œå¥³æ–¹ä¸ç”·æ–¹ä¸¤æƒ…ç›¸æ‚¦ï¼Œå®šæœ‰å©šçº¦ï¼Œå¥³æ–¹çˆ¶äº²ä¸ºäº†æ›´å¤šçš„å½©ç¤¼å°†å¥³å„¿è®¸é…å¦ä¸€äººï¼Œç”·æ–¹çˆ¶äº²å¸¦é¢†å©šç¤¼æŠ¢äº²ï¼Œé©¬é”¡äº”åˆ¤å©šå§»æœ‰æ•ˆï¼Œç”·æ–¹çˆ¶äº²åˆ¤å¾’åˆ‘åŠå¹´ï¼Œå¥³æ–¹çˆ¶äº²åˆ¤åŠ³å½¹åŠå¹´ï¼Œ\n",
    "åˆ¤å†³ä¸€å‡ºï¼Œç¾¤ä¼—æ— ä¸äº¤å£ç§°èµã€‚å…³äºé©¬é”¡äº”å®¡åˆ¤ï¼Œä»¥ä¸‹å“ªä¸€è¯´æ³•æ˜¯é”™è¯¯çš„ï¼Ÿ\\n\n",
    "Aï¼šç¾¤ä¼—ä¸èƒ½ç†è§£çš„åˆ¤å†³ï¼Œå¾ˆéš¾å…·æœ‰å…¬ä¿¡åŠ›\\n\n",
    "B:å¸æ³•è£åˆ¤è¦åšæŒèµ°ç¾¤ä¼—è·¯çº¿\\n\n",
    "C:é©¬é”¡äº”çŸ¥æ°‘æƒ…çŸ¥æ°‘æ„\\n\n",
    "D:åªè¦ç¾¤ä¼—æ»¡æ„ï¼Œä¸å¿…æªå®ˆæ³•å¾‹\\n\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "content = tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[1]:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c79330-9f80-49c0-b8fb-259f0117b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=21137457152 total=25217466368 limit=25769803776 usage=4018275328\n",
      "[4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=21670133760 total=25217466368 limit=25769803776 usage=3485598720\n",
      "Loading checkpoint shards:   0%|                                        | 0/2 [00:00<?, ?it/s][4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=21137457152 total=25217466368 limit=25769803776 usage=4018275328\n",
      "[4pdvGPU Msg(69350:140645045749568:memory.c:566)]: orig free=21670133760 total=25217466368 limit=25769803776 usage=3485598720\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.23s/it]\n",
      "[4pdvGPU Warn(69916:140369235285824:hook.c:475)]: remap handles for device 0\n",
      "[4pdvGPU Warn(69916:140369235285824:hook.c:475)]: remap handles for device 1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../model/law_3b_f16\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer_cot = AutoTokenizer.from_pretrained(model_name)\n",
    "model_cot = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377231eb-6462-4187-a2c1-36f92e2abc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = \"./mcq_sing_nje.csv\"\n",
    "df = pd.read_csv(data)\n",
    "answer = []\n",
    "acc = 0\n",
    "for i in range(len(df)):\n",
    "    prompt = f\"\"\"\n",
    "    è¿™æ˜¯ä¸€ä¸ªå…³äºæ³•å¾‹é—®é¢˜çš„å•é¡¹é€‰æ‹©é¢˜,è¯·æ ¹æ®é¢˜ç›®é€‰æ‹©æ­£ç¡®çš„é€‰é¡¹(åªéœ€è¦å›ç­”æ­£ç¡®é€‰é¡¹å³å¯ï¼Œä¸éœ€è¦è¿›è¡Œåˆ†æ)ï¼š\\n\n",
    "    é—®é¢˜ï¼š{df[\"input\"][i]}\\n\n",
    "    Aï¼š{df[\"A\"][i]}\\n\n",
    "    B:{df[\"B\"][i]}\\n\n",
    "    C:{df[\"C\"][i]}\\n\n",
    "    D:{df[\"D\"][i]}\\n\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # conduct text completion\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    \n",
    "    content = tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[1]:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    print(\"content:\", content)\n",
    "    if content[0] ==  df[\"output\"][i]:\n",
    "        acc += 1    \n",
    "    answer.append(content[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "049c3994-b606-4e9a-9588-27fff504876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    D\n",
       "1    D\n",
       "2    C\n",
       "3    D\n",
       "4    B\n",
       "5    A\n",
       "6    D\n",
       "7    C\n",
       "8    A\n",
       "9    A\n",
       "Name: output, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"output\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b563e0a5-7708-4650-97fa-d18b2310ca24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: D:åªè¦ç¾¤ä¼—æ»¡æ„ï¼Œä¸å¿…æªå®ˆæ³•å¾‹\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"\n",
    "è¿™æ˜¯ä¸€ä¸ªå…³äºæ³•å¾‹é—®é¢˜çš„å•é¡¹é€‰æ‹©é¢˜,è¯·æ ¹æ®é¢˜ç›®é€‰æ‹©æ­£ç¡®çš„é€‰é¡¹(åªéœ€è¦ç»™å‡ºæ­£ç¡®é€‰é¡¹å³å¯)ï¼š\\n\n",
    "é—®é¢˜ï¼šé©¬é”¡äº”åœ¨å®¡ç†â€œæŠ¢äº²æ¡ˆ\"æ—¶å‘ç°ï¼Œå¥³æ–¹ä¸ç”·æ–¹ä¸¤æƒ…ç›¸æ‚¦ï¼Œå®šæœ‰å©šçº¦ï¼Œå¥³æ–¹çˆ¶äº²ä¸ºäº†æ›´å¤šçš„å½©ç¤¼å°†å¥³å„¿è®¸é…å¦ä¸€äººï¼Œç”·æ–¹çˆ¶äº²å¸¦é¢†å©šç¤¼æŠ¢äº²ï¼Œé©¬é”¡äº”åˆ¤å©šå§»æœ‰æ•ˆï¼Œç”·æ–¹çˆ¶äº²åˆ¤å¾’åˆ‘åŠå¹´ï¼Œå¥³æ–¹çˆ¶äº²åˆ¤åŠ³å½¹åŠå¹´ï¼Œ\n",
    "åˆ¤å†³ä¸€å‡ºï¼Œç¾¤ä¼—æ— ä¸äº¤å£ç§°èµã€‚å…³äºé©¬é”¡äº”å®¡åˆ¤ï¼Œä»¥ä¸‹å“ªä¸€è¯´æ³•æ˜¯é”™è¯¯çš„ï¼Ÿ\\n\n",
    "Aï¼šç¾¤ä¼—ä¸èƒ½ç†è§£çš„åˆ¤å†³ï¼Œå¾ˆéš¾å…·æœ‰å…¬ä¿¡åŠ›\\n\n",
    "B:å¸æ³•è£åˆ¤è¦åšæŒèµ°ç¾¤ä¼—è·¯çº¿\\n\n",
    "C:é©¬é”¡äº”çŸ¥æ°‘æƒ…çŸ¥æ°‘æ„\\n\n",
    "D:åªè¦ç¾¤ä¼—æ»¡æ„ï¼Œä¸å¿…æªå®ˆæ³•å¾‹\\n\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer_cot.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer_cot([text], return_tensors=\"pt\").to(model_cot.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model_cot.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "content = tokenizer_cot.decode(generated_ids[0][model_inputs.input_ids.shape[1]:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62755bc-15d0-422b-bb45-406e61459e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (709890214.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [model_inputs.input_ids.shape[1]:]\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[model_inputs.input_ids.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73da0591-7c67-4be2-b6bf-223c8881d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\\nè¿™æ˜¯ä¸€ä¸ªå…³äºæ³•å¾‹é—®é¢˜çš„å•é¡¹é€‰æ‹©é¢˜,è¯·æ ¹æ®é¢˜ç›®é€‰æ‹©æ­£ç¡®çš„é€‰é¡¹(åªéœ€è¦ç»™å‡ºæ­£ç¡®é€‰é¡¹å³å¯)ï¼š\\n\\né—®é¢˜ï¼šé©¬é”¡äº”åœ¨å®¡ç†â€œæŠ¢äº²æ¡ˆ\"æ—¶å‘ç°ï¼Œå¥³æ–¹ä¸ç”·æ–¹ä¸¤æƒ…ç›¸æ‚¦ï¼Œå®šæœ‰å©šçº¦ï¼Œå¥³æ–¹çˆ¶äº²ä¸ºäº†æ›´å¤šçš„å½©ç¤¼å°†å¥³å„¿è®¸é…å¦ä¸€äººï¼Œç”·æ–¹çˆ¶äº²å¸¦é¢†å©šç¤¼æŠ¢äº²ï¼Œé©¬é”¡äº”åˆ¤å©šå§»æœ‰æ•ˆï¼Œç”·æ–¹çˆ¶äº²åˆ¤å¾’åˆ‘åŠå¹´ï¼Œå¥³æ–¹çˆ¶äº²åˆ¤åŠ³å½¹åŠå¹´ï¼Œ\\nåˆ¤å†³ä¸€å‡ºï¼Œç¾¤ä¼—æ— ä¸äº¤å£ç§°èµã€‚å…³äºé©¬é”¡äº”å®¡åˆ¤ï¼Œä»¥ä¸‹å“ªä¸€è¯´æ³•æ˜¯é”™è¯¯çš„ï¼Ÿ\\n\\nAï¼šç¾¤ä¼—ä¸èƒ½ç†è§£çš„åˆ¤å†³ï¼Œå¾ˆéš¾å…·æœ‰å…¬ä¿¡åŠ›\\n\\nB:å¸æ³•è£åˆ¤è¦åšæŒèµ°ç¾¤ä¼—è·¯çº¿\\n\\nC:é©¬é”¡äº”çŸ¥æ°‘æƒ…çŸ¥æ°‘æ„\\n\\nD:åªè¦ç¾¤ä¼—æ»¡æ„ï¼Œä¸å¿…æªå®ˆæ³•å¾‹\\n\\n<|im_end|>\\n<|im_start|>assistant\\nD:åªè¦ç¾¤ä¼—æ»¡æ„ï¼Œä¸å¿…æªå®ˆæ³•å¾‹<|im_end|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_cot.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed077720-5728-4cb0-bc57-3cfe7e7d1fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
